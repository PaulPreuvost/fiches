# Big Data : Data Ingestion (Jour 2)
## Module 3BIGF : Big Data - Data Ingestion
---
## 🔄 1. Définition : Data Ingestion

L’ingestion des données est le processus d’acquisition et d’importation des données depuis diverses sources vers une destination commune pour une analyse en temps réel ou différé.

#### Pourquoi c’est important ?

    ✅ Garantit des décisions basées sur des données fiables
    ✅ Permet la gestion des grandes volumétries
    ✅ Optimise le traitement des données

---
## 🔥 2. Types d'Ingestion des Données
###  Ingestion Batch

- Données collectées à intervalles réguliers
- Utilisé pour les analyses différées (ex. rapports financiers)
- Méthodes : ETL et ELT

###  Ingestion en Streaming

- Données ingérées en temps réel
- Adapté aux applications nécessitant une faible latence (ex. IoT, détection de fraudes)
- Méthodes : Kafka, Spark Streaming
---
## 🏗 3. Approches de l’Ingestion Batch
###  🔹 ETL (Extract, Transform, Load)

- Extract : Extraction des données (SQL, API, fichiers plats)
- Transform : Nettoyage et structuration des données
- Load : Chargement des données transformées dans une base de données

###  🔹 ELT (Extract, Load, Transform)

- Extract : Extraction des données brutes
- Load : Chargement immédiat dans un Data Lake
- Transform : Transformation post-ingestion, utilisant la puissance du Cloud Computing
---
## 🚧 4. Défis de l’Ingestion des Données

    📊 Hétérogénéité des formats (CSV, JSON, XML, vidéo, etc.)
    ❌ Gestion des erreurs et des données corrompues
    📈 Scalabilité pour traiter des volumes massifs
    🔒 Sécurité et conformité (RGPD, HIPAA)
---
## ✅ 5. Bonnes Pratiques

    🔹 Standardiser les formats de données
    🔹 Automatiser les pipelines d’ingestion
    🔹 Implémenter des mécanismes de reprise après erreur
    🔹 Surveiller et analyser la performance de l’ingestion

---
## 🔧 6. Outils et Frameworks
### 🛠️ Outils classiques (ETL)

- Talend, Informatica, Semarchy XDi
- Scripting personnalisé (Python, Bash, SQL)
---

### 🌐 Outils modernes (ELT & Cloud)

- Airbyte : Synchronisation de données entre sources et destinations
- Kafka : Streaming de données en temps réel
- Dbt : Transformation des données après ingestion
- Google BigQuery / Snowflake : Entrepôts de données cloud