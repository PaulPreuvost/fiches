# Big Data : Data Processing avec Apache Spark (Jour 3)
## Module 3BIGF : Big Data - Data Processing & Transformation
---
## ğŸ”¥ 1. Apache Spark : DÃ©finition & Concepts
### ğŸ”¹ DÃ©finition

Apache Spark est un moteur de traitement distribuÃ© conÃ§u pour manipuler des grandes volumÃ©tries de donnÃ©es Ã  haute vitesse.
### ğŸ”¹ CaractÃ©ristiques

    âœ… Traitement en mÃ©moire pour minimiser la latence
    âœ… Support multi-langage : Python, Scala, Java, R
    âœ… RÃ©silience grÃ¢ce aux RDD (Resilient Distributed Datasets)
    âœ… Supporte le batch, le streaming, le machine learning et SQL


---
## ğŸ”„ 2. Cas dâ€™usage

    âœ… Analyse de logs pour dÃ©tecter des erreurs
    âœ… Recommandations personnalisÃ©es (ex. Netflix, Amazon)
    âœ… Traitement en temps rÃ©el (ex. donnÃ©es IoT, monitoring)

---
## ğŸ‘¨ 3. Concepts ClÃ©s : RDD, DataFrame, Dataset
### ğŸ“Œ RDD (Resilient Distributed Dataset)

- Collection distribuÃ©e dâ€™objets immuables
- TolÃ©rance aux pannes (recrÃ©ation via lineage)
- Traitement parallÃ¨le massif
- Deux types dâ€™opÃ©rations :
    - Transformations (map, filter, join)
    - Actions (collect, count, take, reduce)

### ğŸ“Œ DataFrame

- Structure tabulaire avec colonnes et lignes
- API optimisÃ©e pour le traitement des donnÃ©es massives
- Approche recommandÃ©e pour les cas dâ€™usage modernes

### ğŸ“Œ Dataset

- Fusion entre RDD et DataFrame
- Manipulation optimisÃ©e et typage fort

### ğŸ”¹ DiffÃ©rence entre RDD, DataFrame et Dataset :

| Type       | Structure                 | Optimisation  | FacilitÃ© d'utilisation |
|------------|---------------------------|--------------|------------------------|
| **RDD**    | DonnÃ©es brutes (objets)    | Faible       | Complexe               |
| **DataFrame** | Tableau avec colonnes  | TrÃ¨s optimisÃ© | Facile                 |
| **Dataset**  | Tableau avec typage fort | Moyen        | IntermÃ©diaire          |


---
## ğŸ¤“    4. Principaux Composants de Spark
### ğŸš€ Spark Core

- Base de Spark, gÃ¨re le calcul distribuÃ©
- Fournit lâ€™API RDD et gestion des ressources

### ğŸ—ƒï¸ Spark SQL

- ExÃ©cute des requÃªtes SQL sur des donnÃ©es distribuÃ©es
- Compatible avec JSON, Parquet, Avro, CSV, HDFS, S3, JDBC

Avantages : 
    
    âœ… FacilitÃ© d'utilisation (utilisation de SQL)
    âœ… Haute performance (optimisation via Catalyst)

### âš¡ Spark Streaming

- Traitement temps rÃ©el des flux de donnÃ©es
- ModÃ¨le de micro-batch
- IntÃ©gration avec Kafka, Flume, HDFS

### ğŸ¤– MLlib (Machine Learning)

BibliothÃ¨que ML distribuÃ©e
Algorithmes supportÃ©s : classification, clustering, rÃ©gression

Exemples : 

    âœ… Classification : DÃ©tection de spam
    âœ… Clustering : Regroupement de clients
    âœ… RÃ©gression : PrÃ©diction des prix immobiliers

### ğŸ”— GraphX

- Traitement de graphes distribuÃ©s
- UtilisÃ© pour lâ€™analyse des relations entre objets (ex. rÃ©seaux sociaux)
---
## ğŸ 5. Spark & PySpark

âœ… PySpark : Interface Python pour Apache Spark

âœ… Commandes utiles :

    pyspark : Lancer Spark en mode shell Python
    spark-shell : Lancer Spark avec Scala
    sparkr : Lancer Spark avec R

### ğŸ”¹ PrÃ©requis pour PySpark :

- Java 8 ou supÃ©rieur
- Python 3.x
- Installation de Spark
