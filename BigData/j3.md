# Big Data : Data Processing avec Apache Spark (Jour 3)
## Module 3BIGF : Big Data - Data Processing & Transformation
---
## 🔥 1. Apache Spark : Définition & Concepts
### 🔹 Définition

Apache Spark est un moteur de traitement distribué conçu pour manipuler des grandes volumétries de données à haute vitesse.
### 🔹 Caractéristiques

    ✅ Traitement en mémoire pour minimiser la latence
    ✅ Support multi-langage : Python, Scala, Java, R
    ✅ Résilience grâce aux RDD (Resilient Distributed Datasets)
    ✅ Supporte le batch, le streaming, le machine learning et SQL


---
## 🔄 2. Cas d’usage

    ✅ Analyse de logs pour détecter des erreurs
    ✅ Recommandations personnalisées (ex. Netflix, Amazon)
    ✅ Traitement en temps réel (ex. données IoT, monitoring)

---
## 👨 3. Concepts Clés : RDD, DataFrame, Dataset
### 📌 RDD (Resilient Distributed Dataset)

- Collection distribuée d’objets immuables
- Tolérance aux pannes (recréation via lineage)
- Traitement parallèle massif
- Deux types d’opérations :
    - Transformations (map, filter, join)
    - Actions (collect, count, take, reduce)

### 📌 DataFrame

- Structure tabulaire avec colonnes et lignes
- API optimisée pour le traitement des données massives
- Approche recommandée pour les cas d’usage modernes

### 📌 Dataset

- Fusion entre RDD et DataFrame
- Manipulation optimisée et typage fort

### 🔹 Différence entre RDD, DataFrame et Dataset :

| Type       | Structure                 | Optimisation  | Facilité d'utilisation |
|------------|---------------------------|--------------|------------------------|
| **RDD**    | Données brutes (objets)    | Faible       | Complexe               |
| **DataFrame** | Tableau avec colonnes  | Très optimisé | Facile                 |
| **Dataset**  | Tableau avec typage fort | Moyen        | Intermédiaire          |


---
## 🤓    4. Principaux Composants de Spark
### 🚀 Spark Core

- Base de Spark, gère le calcul distribué
- Fournit l’API RDD et gestion des ressources

### 🗃️ Spark SQL

- Exécute des requêtes SQL sur des données distribuées
- Compatible avec JSON, Parquet, Avro, CSV, HDFS, S3, JDBC

Avantages : 
    
    ✅ Facilité d'utilisation (utilisation de SQL)
    ✅ Haute performance (optimisation via Catalyst)

### ⚡ Spark Streaming

- Traitement temps réel des flux de données
- Modèle de micro-batch
- Intégration avec Kafka, Flume, HDFS

### 🤖 MLlib (Machine Learning)

Bibliothèque ML distribuée
Algorithmes supportés : classification, clustering, régression

Exemples : 

    ✅ Classification : Détection de spam
    ✅ Clustering : Regroupement de clients
    ✅ Régression : Prédiction des prix immobiliers

### 🔗 GraphX

- Traitement de graphes distribués
- Utilisé pour l’analyse des relations entre objets (ex. réseaux sociaux)
---
## 🐍 5. Spark & PySpark

✅ PySpark : Interface Python pour Apache Spark

✅ Commandes utiles :

    pyspark : Lancer Spark en mode shell Python
    spark-shell : Lancer Spark avec Scala
    sparkr : Lancer Spark avec R

### 🔹 Prérequis pour PySpark :

- Java 8 ou supérieur
- Python 3.x
- Installation de Spark
